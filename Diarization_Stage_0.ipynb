{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 36px; color: #3498db; font-weight: bold;\">Prosody Tools</h1>\n",
    "\n",
    "## <h2 style=\"text-align: center; font-size: 28px; color: #2ecc71; font-weight: bold;\">Prosody Diarization Tool</h2>\n",
    "### <h3 style=\"text-align: center; font-size: 24px; color: #e74c3c; font-family: 'Arial', sans-serif; font-weight: bold;\">Stage 0</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Project: Prosody\n",
    "\n",
    "Institution: Reutlingen University\n",
    "\n",
    "Author: Mohamad Eyad Alkostanini\n",
    "\n",
    "Supervisor: Bakir Hadzic, Parvez Mohammed, (ViSiR)\n",
    "\n",
    "Professor: Prof. Matthias RÃ¤tsch\n",
    "\n",
    "Description:\n",
    "This script performs diarization as part of the Prosody in Mental Health project.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All folders created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List of folder paths to create\n",
    "folders = [\n",
    "    r\".\\Database\\0.Interviews\\mp4\",\n",
    "    \n",
    "    r\".\\Database\\0.Interviews\\wav\\stage_0\", # wav files for the first stage (first try to diarize the files)\n",
    "    r\".\\Database\\1.rttms\\stage_0\", # the diarization text files as '.rttm'\n",
    "    r\".\\Database\\2.Speaker_Parts\\Stage_0\", # the parts for each speaker\n",
    "    r\".\\Database\\3.Speaker_Combined\\Stage_0\", # the combined parts for each speaker\n",
    "    \n",
    "    r\".\\Database\\0.Interviews\\wav\\stage_1\",\n",
    "    r\".\\Database\\1.rttms\\stage_1\",\n",
    "    r\".\\Database\\2.Speaker_Parts\\Stage_1\",\n",
    "    r\".\\Database\\3.Speaker_Combined\\Stage_1\"\n",
    "]\n",
    "\n",
    "# Create folders if they do not exist\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"All folders created successfully!\")\n",
    "\n",
    "# if the Intertviews are mp4 \n",
    "# if the interviewws are wav "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert mp4 to wav (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in .\\Database\\0.Interviews\\wav\\stage_0\\EXPRA_1_003.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing audio in .\\Database\\0.Interviews\\wav\\stage_0\\EXPRA_3_012.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def convert_video_to_audio(input_video, output_audio):\n",
    "    video_clip = VideoFileClip(input_video)\n",
    "    audio_clip = video_clip.audio\n",
    "    audio_clip.write_audiofile(output_audio, codec='pcm_s16le', bitrate='16k')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = r\".\\Database\\0.Interviews\\mp4\"\n",
    "    output_dir = r\".\\Database\\0.Interviews\\wav\\stage_0\" #the wav files \n",
    "\n",
    "    if not os.path.exists(input_dir):\n",
    "        os.makedirs(input_dir)\n",
    "        \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file in os.listdir(input_dir):\n",
    "        if file.endswith(\".mp4\"):\n",
    "            input_video = os.path.join(input_dir, file)\n",
    "            output_audio = os.path.join(output_dir, os.path.splitext(file)[0] + \".wav\")\n",
    "            convert_video_to_audio(input_video, output_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyannote pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "WARNING:py.warnings:c:\\Users\\Alkostantini-GIGA\\anaconda3\\envs\\test0\\lib\\inspect.py:746: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "#import torch\n",
    "\n",
    "# instantiate the pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "  \"pyannote/speaker-diarization-3.1\",\n",
    "  use_auth_token=\"here you have to enter your auth-key\") \n",
    "\n",
    "#https://huggingface.co/pyannote/speaker-diarization-3.1\n",
    "#torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the RTTM files\n",
    "## Stage 0, number of speakers are 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:c:\\Users\\Alkostantini-GIGA\\anaconda3\\envs\\test0\\lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1823.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPRA_1_003.wav  Done...\n",
      "EXPRA_3_012.wav  Done...\n"
     ]
    }
   ],
   "source": [
    "# Save RTTMs\n",
    "\n",
    "import os\n",
    "import torchaudio\n",
    "# diarization pipeline\n",
    "diarization_pipeline = pipeline\n",
    "\n",
    "# Path to WAV files\n",
    "input_dir = r\".\\Database\\0.Interviews\\wav\\stage_0\"\n",
    "\n",
    "# Create output if it doesn't exist\n",
    "output_dir = r\".\\Database\\1.rttms\\stage_0\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterateall WAV files in the input\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".wav\"):\n",
    "        # Construct the full path to the WAV file\n",
    "        wav_file_path = os.path.join(input_dir, file_name)\n",
    "\n",
    "        # Perform diarization\n",
    "        #waveform, sample_rate = torchaudio.load(wav_file_path)\n",
    "        #diarization = pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate})\n",
    "\n",
    "        diarization = pipeline(wav_file_path, num_speakers=2) # number of speakers\n",
    " \n",
    "        \n",
    "        # Construct output RTTM file\n",
    "        rttm_file_path = os.path.join(output_dir, f\"{os.path.splitext(file_name)[0]}.rttm\")\n",
    "\n",
    "        # diarization output RTTM format\n",
    "        with open(rttm_file_path, \"w\") as rttm_file:\n",
    "            diarization.write_rttm(rttm_file)\n",
    "            print (file_name, ' Done...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Alkostantini-GIGA\\AppData\\Local\\Temp\\ipykernel_22452\\1786308567.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat([combined_df, rttm_df], ignore_index=True)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Interview_ID</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Start</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Speaker_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPEAKER</td>\n",
       "      <td>EXPRA_1_003</td>\n",
       "      <td>1</td>\n",
       "      <td>2.377</td>\n",
       "      <td>7.763</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPEAKER</td>\n",
       "      <td>EXPRA_1_003</td>\n",
       "      <td>1</td>\n",
       "      <td>10.966</td>\n",
       "      <td>3.510</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPEAKER</td>\n",
       "      <td>EXPRA_1_003</td>\n",
       "      <td>1</td>\n",
       "      <td>15.539</td>\n",
       "      <td>0.253</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPEAKER</td>\n",
       "      <td>EXPRA_1_003</td>\n",
       "      <td>1</td>\n",
       "      <td>17.277</td>\n",
       "      <td>1.789</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPEAKER</td>\n",
       "      <td>EXPRA_1_003</td>\n",
       "      <td>1</td>\n",
       "      <td>21.327</td>\n",
       "      <td>2.869</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>SPEAKER</td>\n",
       "      <td>EXPRA_3_012</td>\n",
       "      <td>1</td>\n",
       "      <td>663.556</td>\n",
       "      <td>9.096</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>SPEAKER</td>\n",
       "      <td>EXPRA_3_012</td>\n",
       "      <td>1</td>\n",
       "      <td>670.525</td>\n",
       "      <td>0.321</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>SPEAKER</td>\n",
       "      <td>EXPRA_3_012</td>\n",
       "      <td>1</td>\n",
       "      <td>673.057</td>\n",
       "      <td>7.239</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>SPEAKER</td>\n",
       "      <td>EXPRA_3_012</td>\n",
       "      <td>1</td>\n",
       "      <td>680.937</td>\n",
       "      <td>0.658</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>SPEAKER</td>\n",
       "      <td>EXPRA_3_012</td>\n",
       "      <td>1</td>\n",
       "      <td>681.950</td>\n",
       "      <td>3.662</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>366 rows Ã 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Type Interview_ID Channel    Start  Duration  Speaker_ID\n",
       "0    SPEAKER  EXPRA_1_003       1    2.377     7.763  SPEAKER_01\n",
       "1    SPEAKER  EXPRA_1_003       1   10.966     3.510  SPEAKER_01\n",
       "2    SPEAKER  EXPRA_1_003       1   15.539     0.253  SPEAKER_01\n",
       "3    SPEAKER  EXPRA_1_003       1   17.277     1.789  SPEAKER_01\n",
       "4    SPEAKER  EXPRA_1_003       1   21.327     2.869  SPEAKER_01\n",
       "..       ...          ...     ...      ...       ...         ...\n",
       "361  SPEAKER  EXPRA_3_012       1  663.556     9.096  SPEAKER_00\n",
       "362  SPEAKER  EXPRA_3_012       1  670.525     0.321  SPEAKER_01\n",
       "363  SPEAKER  EXPRA_3_012       1  673.057     7.239  SPEAKER_00\n",
       "364  SPEAKER  EXPRA_3_012       1  680.937     0.658  SPEAKER_00\n",
       "365  SPEAKER  EXPRA_3_012       1  681.950     3.662  SPEAKER_00\n",
       "\n",
       "[366 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# directory RTTM files\n",
    "rttm_dir = r\".\\Database\\1.rttms\\Stage_0\"\n",
    "\n",
    "columns = [\"Type\", \"Interview_ID\", \"Channel\", \"Start\", \"Duration\", \"NA1\", \"NA2\", \"Speaker_ID\", \"NA3\", \"NA4\"]\n",
    "\n",
    "# combined data from all RTTM files\n",
    "combined_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# RTTM files in the directory\n",
    "for file_name in os.listdir(rttm_dir):\n",
    "    if file_name.endswith(\".rttm\"):\n",
    "        # path to the RTTM file\n",
    "        rttm_file_path = os.path.join(rttm_dir, file_name)\n",
    "\n",
    "        # Read the RTTM file into a DataFrame\n",
    "        rttm_df = pd.read_csv(rttm_file_path, sep=\" \", header=None, names=columns)\n",
    "\n",
    "        # Append RTTM file to the combined DataFrame\n",
    "        combined_df = pd.concat([combined_df, rttm_df], ignore_index=True)\n",
    "\n",
    "# Drop the columns 'NA1', 'NA2', 'NA3', and 'NA4' from the DataFrame\n",
    "combined_df = combined_df.drop(columns=['NA1', 'NA2', 'NA3', 'NA4'])\n",
    "\n",
    "combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SPEAKER_01', 'SPEAKER_00'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['Speaker_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EXPRA_1_003', 'EXPRA_3_012'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['Interview_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# split based on start and duration\n",
    "def split_audio(input_path, output_path, start, duration, filename):\n",
    "    sound = AudioSegment.from_wav(input_path)\n",
    "    split_sound = sound[start*1000:(start+duration)*1000]  # Pydub in milliseconds\n",
    "    split_sound.export(os.path.join(output_path, filename), format=\"wav\")\n",
    "\n",
    "df = combined_df\n",
    "\n",
    "input_dir = r\".\\Database\\0.Interviews\\wav\\Stage_0\"\n",
    "output_dir = r\".\\Database\\2.Speaker_Parts\\Stage_0\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "i = 0\n",
    "# each row in df\n",
    "for index, row in df.iterrows():\n",
    "    interview_id = row['Interview_ID']\n",
    "    speaker_id = row['Speaker_ID']\n",
    "    start = row['Start']\n",
    "    duration = row['Duration']\n",
    "    filename = f\"{interview_id}_{speaker_id}_{i}.wav\"\n",
    "    input_path = os.path.join(input_dir, f\"{interview_id}.wav\")\n",
    "    output_path = os.path.join(output_dir, speaker_id)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    split_audio(input_path, output_path, start, duration, filename)\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join the Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SPEAKER_00...\n",
      "Combined file saved: .\\Database\\3.Speaker_Combined\\Stage_0\\SPEAKER_00\\EXPRA_1_003_SPEAKER_00.wav\n",
      "Combined file saved: .\\Database\\3.Speaker_Combined\\Stage_0\\SPEAKER_00\\EXPRA_3_012_SPEAKER_00.wav\n",
      "Processing SPEAKER_01...\n",
      "Combined file saved: .\\Database\\3.Speaker_Combined\\Stage_0\\SPEAKER_01\\EXPRA_1_003_SPEAKER_01.wav\n",
      "Combined file saved: .\\Database\\3.Speaker_Combined\\Stage_0\\SPEAKER_01\\EXPRA_3_012_SPEAKER_01.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wave\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Function to concatenate wav files with a gap\n",
    "def concatenate_wav_files_with_gap(directory, output, gap_duration=0.1):\n",
    "    # Store wave file objects\n",
    "    wave_files = {}\n",
    "\n",
    "    # List files in directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            # Prefix based on the first two parts of the filename\n",
    "            if (filename.split('_')[0] == 'PBN'):\n",
    "                prefix = '_'.join(filename.split('_')[:2])\n",
    "            elif (filename.split('_')[0] == 'EXPRA'):\n",
    "                prefix = '_'.join(filename.split('_')[:3])\n",
    "            else:\n",
    "                print('Name ERROR..............')\n",
    "                continue\n",
    "\n",
    "            # Append filename to the corresponding prefix list\n",
    "            if prefix in wave_files:\n",
    "                wave_files[prefix].append(filename)\n",
    "            else:\n",
    "                wave_files[prefix] = [filename]\n",
    "\n",
    "    # Concatenate WAV files with the specified gap\n",
    "    for prefix, files in wave_files.items():\n",
    "        # Sort the files based on the last numeric part in the filename\n",
    "        files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "        # Create an empty AudioSegment to combine files\n",
    "        combined = AudioSegment.silent(duration=0)  # Start with zero duration\n",
    "\n",
    "        # Iterate through each file and concatenate with silence gaps\n",
    "        for file in files:\n",
    "            # Load each WAV file\n",
    "            current_segment = AudioSegment.from_wav(os.path.join(directory, file))\n",
    "            \n",
    "            # Add a 0.2-second silent gap between segments\n",
    "            combined += current_segment + AudioSegment.silent(duration=gap_duration * 1000)\n",
    "\n",
    "        # Export the combined segment to the output directory\n",
    "        output_filename = os.path.join(output, f\"{prefix}_{os.path.basename(directory)}.wav\")\n",
    "        combined.export(output_filename, format=\"wav\")\n",
    "        print(f\"Combined file saved: {output_filename}\")\n",
    "\n",
    "# Define directories and speakers\n",
    "if __name__ == \"__main__\":\n",
    "    speakers = [f\"SPEAKER_{str(i).zfill(2)}\" for i in range(2)]  # Adjust range as needed (e.g., range(10) for 10 speakers)\n",
    "    stage = \"Stage_0\"\n",
    "    \n",
    "    for speaker in speakers:\n",
    "        directory = rf\".\\Database\\2.Speaker_Parts\\{stage}\\{speaker}\"\n",
    "        output = rf\".\\Database\\3.Speaker_Combined\\{stage}\\{speaker}\"\n",
    "        # Create the output directory if it does not exist\n",
    "        os.makedirs(output, exist_ok=True)\n",
    "\n",
    "        print(f\"Processing {speaker}...\")\n",
    "        # Run the concatenation function with the desired gap duration\n",
    "        concatenate_wav_files_with_gap(directory, output, gap_duration=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
